import os
import zipfile
import json
import pandas as pd
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig
from torch.optim import AdamW
from tqdm import tqdm

# 1. ZIP í•´ì œ í•¨ìˆ˜
def unzip_file(zip_path, extract_to):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_to)
    print(f"ğŸ“‚ {zip_path} ì••ì¶• í•´ì œ ì™„ë£Œ â†’ {extract_to}")

# 2. ì¤‘ì²© ZIP í•´ì œ
def unzip_nested_zip_files(folder_path):
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith('.zip'):
                zip_path = os.path.join(root, file)
                extract_to = os.path.splitext(zip_path)[0]

                try:
                    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                        zip_ref.extractall(extract_to)
                    print(f"ğŸ“¦ ì¤‘ì²© ZIP í•´ì œ: {zip_path} â†’ {extract_to}")
                except Exception as e:
                    print(f"âš ï¸ {zip_path} í•´ì œ ì‹¤íŒ¨: {e}")

# 3. JSON ë¡œë“œ ë° ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜
def extract_newscontent_from_json(directory):
    result = []
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith('.json'):
                try:
                    with open(os.path.join(root, file), 'r', encoding='utf-8') as f:
                        data = json.load(f)

                        content = data.get('sourceDataInfo', {}).get('newsContent', '')
                        if not content:
                            sentences = data.get('labeledDataInfo', {}).get('referSentenceInfo', [])
                            if isinstance(sentences, list):
                                content = " ".join(
                                    s.get("sentenceContent", "") for s in sentences if isinstance(s, dict)
                                )

                        if content.strip():
                            result.append({'text': content})
                except Exception as e:
                    print(f"âš ï¸ JSON ë¡œë“œ ì‹¤íŒ¨: {file}, ì´ìœ : {e}")
    return result

# 4. ë©”ì¸ í•¨ìˆ˜
def main():
    # ì••ì¶• íŒŒì¼ ê²½ë¡œ
    clickbait_zip = "/content/drive/MyDrive/ColabNotebooks/Clickbait.zip"
    nonclickbait_zip = "/content/drive/MyDrive/ColabNotebooks/NonClickbait.zip"
    clickbait_dir = "/content/Clickbait"
    nonclickbait_dir = "/content/NonClickbait"

    # ì••ì¶• í•´ì œ
    print("ğŸ”½ ZIP ì••ì¶• í•´ì œ ì¤‘...")
    unzip_file(clickbait_zip, clickbait_dir)
    unzip_file(nonclickbait_zip, nonclickbait_dir)

    print("ğŸ”½ ì¤‘ì²© ZIP íŒŒì¼ í•´ì œ ì¤‘...")
    unzip_nested_zip_files(clickbait_dir)
    unzip_nested_zip_files(nonclickbait_dir)

    # Clickbait (ê°€ì§œ ë‰´ìŠ¤)
    print("ğŸ“‚ Clickbait â†’ ê°€ì§œ ë‰´ìŠ¤ ë¡œë“œ ì¤‘...")
    fake_data = extract_newscontent_from_json(clickbait_dir)
    if not fake_data:
        raise ValueError("âŒ Clickbait ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨.")
    fake_df = pd.DataFrame(fake_data)
    fake_df['label'] = 1
    print("âœ… ê°€ì§œ ë‰´ìŠ¤ DataFrame ìƒì„± ì™„ë£Œ:", fake_df.shape)

    # NonClickbait (ì§„ì§œ ë‰´ìŠ¤)
    print("ğŸ“‚ NonClickbait â†’ ì§„ì§œ ë‰´ìŠ¤ ë¡œë“œ ì¤‘...")
    real_data = extract_newscontent_from_json(nonclickbait_dir)
    if not real_data:
        raise ValueError("âŒ NonClickbait ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨.")
    real_df = pd.DataFrame(real_data)
    real_df['label'] = 0
    print("âœ… ì§„ì§œ ë‰´ìŠ¤ DataFrame ìƒì„± ì™„ë£Œ:", real_df.shape)

    # ì •ë³´ ì¶œë ¥
    print("ğŸ“° ê°€ì§œ ë‰´ìŠ¤ ìƒ˜í”Œ:\n", fake_df.head())
    print("ğŸ“° ì§„ì§œ ë‰´ìŠ¤ ìƒ˜í”Œ:\n", real_df.head())
    print("ğŸ“Š ë¼ë²¨ ë¶„í¬ (ê°€ì§œ):\n", fake_df['label'].value_counts())
    print("ğŸ“Š ë¼ë²¨ ë¶„í¬ (ì§„ì§œ):\n", real_df['label'].value_counts())

    # ë³‘í•© ë° ì €ì¥
    final_df = pd.concat([fake_df, real_df], ignore_index=True)
    final_df = final_df[final_df['text'].str.strip() != '']
    final_df = final_df.sample(frac=1, random_state=42).reset_index(drop=True)

    output_path = "/content/outputdata.csv"
    final_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"âœ… ìµœì¢… CSV ì €ì¥ ì™„ë£Œ: {output_path}")
    print(final_df.head())

# ì‹¤í–‰
if __name__ == "__main__":
    main()
