import os
import re
import time
import torch
import requests
import pandas as pd
import matplotlib.pyplot as plt
from bs4 import BeautifulSoup
from collections import Counter
from urllib.parse import quote
from dotenv import load_dotenv
from typing import List
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel
from torch.optim import AdamW
from sklearn.model_selection import train_test_split

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
assert NAVER_CLIENT_ID, "â— .env íŒŒì¼ì—ì„œ NAVER_CLIENT_IDë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
assert NAVER_CLIENT_SECRET, "â— .env íŒŒì¼ì—ì„œ NAVER_CLIENT_SECRETë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

# âœ… fetch ìœ í‹¸
def fetch_with_retry(url, headers=None, max_retries=3):
    for _ in range(max_retries):
        try:
            res = requests.get(url, headers=headers, timeout=10)
            if res.status_code == 200:
                return res
        except requests.RequestException:
            time.sleep(1)
    return None

# âœ… Dataset í´ë˜ìŠ¤
class SummaryDataset(Dataset):
    def __init__(self, summaries, labels, tokenizer, max_len=256):
        self.summaries = summaries    
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.summaries)    

    def __getitem__(self, idx):
        text = self.summaries[idx]    
        label = self.labels[idx]
        encoding = self.tokenizer(
          text,
          truncation=True,
          padding='max_length',
          max_length=self.max_len,
          return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'label': torch.tensor(label, dtype=torch.long)
        }

# âœ… ëª¨ë¸ í´ë˜ìŠ¤
class KoBERTClassifier(torch.nn.Module):
    def __init__(self, bert):
        super(KoBERTClassifier, self).__init__()
        self.bert = bert
        self.classifier = torch.nn.Linear(bert.config.hidden_size, 2)  # ì´ì§„ ë¶„ë¥˜

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state[:, 0]
        return self.classifier(pooled_output)

# âœ… í•™ìŠµ ìœ í‹¸
def train_epoch(model, loader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    for i, batch in enumerate(loader):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        optimizer.zero_grad()
        logits = model(input_ids, attention_mask)
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        if (i + 1) % 10 == 0 or (i + 1) == len(loader):
            percent = ((i + 1) / len(loader)) * 100
            print(f"ğŸŒ€ [{i + 1}/{len(loader)}] ({percent:.1f}%) Loss: {loss.item():.4f}")
    return total_loss / len(loader)

def eval_model(model, loader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0
    with torch.no_grad():
        for batch in loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)

            logits = model(input_ids, attention_mask)
            loss = criterion(logits, labels)
            total_loss += loss.item()

            preds = torch.argmax(logits, dim=1)
            correct += (preds == labels).sum().item()
    return total_loss / len(loader), correct / len(loader.dataset)

def plot_training_history(train_losses, val_accuracies):
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Train Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(val_accuracies, label='Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Validation Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.savefig("training_plot.png")
    plt.show()

# âœ… ë°ì´í„° ë¡œë“œ
def load_data_from_csv(path):
    df = pd.read_csv(path)
    texts = df['text'].tolist()
    labels = df['label'].tolist()
    return list(zip(texts, labels))

# âœ… ì „ì²˜ë¦¬
def prepare_rag_training_data(raw_data, force=False):
    if not raw_data:
        return [], []
    texts, labels = zip(*raw_data)
    return list(texts), list(labels)

    print("ğŸ§ª ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    print(f"ì´ í•™ìŠµ ë°ì´í„° ìˆ˜: {len(train_dataset)}")
    print(f"ë°°ì¹˜ í¬ê¸°: {train_loader.batch_size}, ì´ ë°°ì¹˜ ìˆ˜: {len(train_loader)}")

# âœ… ë©”ì¸ ì‹¤í–‰
def main(force=False):
    print("ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    raw_data = load_data_from_csv("outputdata.csv")
    texts, labels = prepare_rag_training_data(raw_data, force=force)
    if not texts:
        print("â— í•™ìŠµí•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        return

    print("ğŸ“Š í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• ...")
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        texts, labels, test_size=0.2, random_state=42
    )

    print("ğŸ”¤ í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°...")
    tokenizer = BertTokenizer.from_pretrained('beomi/kcbert-base')
    bert_model = BertModel.from_pretrained('beomi/kcbert-base')

    train_dataset = SummaryDataset(train_texts, train_labels, tokenizer)
    val_dataset = SummaryDataset(val_texts, val_labels, tokenizer)
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=16)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = KoBERTClassifier(bert_model).to(device)
    optimizer = AdamW(model.parameters(), lr=2e-5)
    criterion = torch.nn.CrossEntropyLoss()

    train_losses, val_accuracies = [], []
    epochs = 3
    for epoch in range(epochs):
        print(f"\nğŸ“˜ Epoch {epoch + 1}/{epochs}")
        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
        val_loss, val_acc = eval_model(model, val_loader, criterion, device)

        print(f"âœ… Training Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f} | ğŸ” Validation Acc: {val_acc:.4f}")
        train_losses.append(train_loss)
        val_accuracies.append(val_acc)

        torch.save(model.state_dict(), f"rag_classifier_model.pt")
        print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: rag_classifier_model.pt")

    plot_training_history(train_losses, val_accuracies)

if __name__ == "__main__":
    main()
