import os
import re
import time
import requests
import pandas as pd
from typing import List
from urllib.parse import quote
from bs4 import BeautifulSoup
import torch
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from dotenv import load_dotenv
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration, BertTokenizer, BertModel
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from collections import Counter

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
    raise ValueError("ë„¤ì´ë²„ API í‚¤ê°€ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

# ì¬ì‹œë„ í•¨ìˆ˜
def fetch_with_retry(url, headers, retries=3, timeout=15, delay=2):
    for i in range(retries):
        try:
            res = requests.get(url, headers=headers, timeout=timeout)
            res.raise_for_status()
            return res
        except requests.exceptions.RequestException as e:
            print(f"ğŸ”´ ìš”ì²­ ì‹¤íŒ¨ {url} (ì‹œë„ {i+1}/{retries}): {e}")
            if i < retries - 1:
                time.sleep(delay)
            else:
                return None

# ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
class NaverNewsCrawler:
    def __init__(self):
        self.client_id = os.getenv("NAVER_CLIENT_ID")
        self.client_secret = os.getenv("NAVER_CLIENT_SECRET")
        self.headers = {
            "X-Naver-Client-Id": self.client_id,
            "X-Naver-Client-Secret": self.client_secret
        }

    def extract_keywords(self, text: str) -> List[str]:
        words = re.findall(r'\b[ê°€-í£]{2,}\b', text)
        if not words:
            return ["ë‰´ìŠ¤"]
        counted = Counter(words)
        return [w for w, c in counted.most_common(5)]

    def search_news_urls(self, keywords: List[str], max_total=20, display=5) -> List[str]:
        if not keywords:
            print("í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        query = quote(" ".join(keywords))
        all_links = set()
        start = 1

        while len(all_links) < max_total and start <= 100:
            url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display={display}&start={start}&sort=sim"
            try:
                res = requests.get(url, headers=self.headers, timeout=10)
                if res.status_code != 200:
                    print(f"ğŸ”´ ë„¤ì´ë²„ API ìš”ì²­ ì‹¤íŒ¨: {res.status_code}")
                    break
                items = res.json().get("items", [])
                if not items:
                    break

                for item in items:
                    link = item.get("link", "")
                    if any(domain in link for domain in ["news.naver.com", "news.sbs.co.kr"]):
                        all_links.add(link)

                if len(items) < display:
                    break

                start += display
                time.sleep(0.3)
            except Exception as e:
                print(f"ğŸ”´ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                break

        return list(all_links)[:max_total]

    def crawl_articles(self, urls: List[str]) -> List[str]:
        articles = []
        for url in urls:
            res = fetch_with_retry(url, headers={"User-Agent": "Mozilla/5.0"})
            if res is None:
                continue
            soup = BeautifulSoup(res.text, "html.parser")
            selectors = [
                "div#newsct_article", "div#articleBodyContents", "div#articeBody",
                "div#newsEndContents", "div#dic_area", "div.article_body"
            ]
            article_text = ""
            for sel in selectors:
                container = soup.select_one(sel)
                if container:
                    article_text = container.get_text(separator=" ", strip=True)
                    break
            if len(article_text) > 100:
                articles.append(article_text)
        return articles

# ìš”ì•½ê¸°
class RagSummarizer:
    def __init__(self, model_name="digit82/kobart-summarization"):
        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)
        self.model = BartForConditionalGeneration.from_pretrained(model_name).to('cuda' if torch.cuda.is_available() else 'cpu')

    def summarize(self, texts: List[str], max_length=128) -> List[str]:
        summaries = []
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        for text in texts:
            inputs = self.tokenizer.encode(text, return_tensors='pt', max_length=1024, truncation=True).to(device)
            summary_ids = self.model.generate(
                inputs, max_length=max_length, min_length=30,
                length_penalty=2.0, num_beams=4, early_stopping=True
            )
            summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)
            summaries.append(summary)
        return summaries

# KoBERT ë¶„ë¥˜ê¸°
class KoBERTClassifier(torch.nn.Module):
    def __init__(self, bert, hidden_size=768, num_classes=2):
        super(KoBERTClassifier, self).__init__()
        self.bert = bert
        self.classifier = torch.nn.Linear(hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        return self.classifier(pooled_output)

# Dataset
class SummaryDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer.encode_plus(
            self.texts[idx],
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'label': torch.tensor(self.labels[idx], dtype=torch.long)
        }

# í•™ìŠµ ë£¨í”„
def train_epoch(model, loader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    for batch in loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)
        optimizer.zero_grad()
        logits = model(input_ids, attention_mask)
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(loader)

def eval_model(model, loader, criterion, device):
    model.eval()
    total_loss, correct, total = 0, 0, 0
    with torch.no_grad():
        for batch in loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)
            logits = model(input_ids, attention_mask)
            loss = criterion(logits, labels)
            total_loss += loss.item()
            correct += (torch.argmax(logits, dim=1) == labels).sum().item()
            total += labels.size(0)
    return total_loss / len(loader), correct / total

# ìš”ì•½ ë°ì´í„° ìƒì„± or ìºì‹œ ë¡œë”© (ì¤‘ë³µ ì œê±° í¬í•¨)
def prepare_rag_training_data(text_label_pairs, cache_path="rag_summaries.csv", force=False):
    if os.path.exists(cache_path) and not force:
        print("âœ… ìš”ì•½ ìºì‹œ íŒŒì¼ ë¡œë”© ì¤‘...")
        df = pd.read_csv(cache_path)
        df = df.drop_duplicates(subset=["summary", "label"])
        return df["summary"].tolist(), df["label"].tolist()

    crawler = NaverNewsCrawler()
    summarizer = RagSummarizer()
    all_summaries, all_labels = [], []

    for raw_text, label in text_label_pairs:
        keywords = crawler.extract_keywords(raw_text)
        urls = crawler.search_news_urls(keywords, max_total=20)
        articles = crawler.crawl_articles(urls)
        if not articles:
            print(f"â— ë‰´ìŠ¤ ì—†ìŒ: {raw_text}")
            continue
        summaries = summarizer.summarize(articles)
        all_summaries.extend(summaries)
        all_labels.extend([label] * len(summaries))

    df = pd.DataFrame({"summary": all_summaries, "label": all_labels})
    df.drop_duplicates(subset=["summary", "label"], inplace=True)
    df.to_csv(cache_path, index=False)
    print("âœ… ìš”ì•½ ë°ì´í„° ìºì‹œì— ì €ì¥ ì™„ë£Œ")
    return df["summary"].tolist(), df["label"].tolist()

def load_data_from_csv(file_path):
    df = pd.read_csv(file_path)
    return list(zip(df['text'].tolist(), df['label'].tolist()))

# ì²´í¬í¬ì¸íŠ¸ ì €ì¥
def save_checkpoint(model, optimizer, epoch, loss, path):
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
    }, path)
    print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {path}")

# ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° (í•™ìŠµ ì¬ê°œìš©)
def load_checkpoint(model, optimizer, path, device):
    checkpoint = torch.load(path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    epoch = checkpoint['epoch']
    loss = checkpoint['loss']
    print(f"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: epoch {epoch}, loss {loss:.4f}")
    return model, optimizer, epoch, loss

# ë©”ì¸ í•¨ìˆ˜
def main(force=False, resume_checkpoint=None):
    raw_data = load_data_from_csv("outputdata.csv")
    print(f"ì›ë³¸ ë°ì´í„° ê°œìˆ˜: {len(raw_data)}")
    print(f"ì¤‘ë³µ ì œê±° í›„ ê°œìˆ˜: {len(set(raw_data))}")
    print("[1] ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ìš”ì•½")
    texts, labels = prepare_rag_training_data(raw_data, force=force)
    if not texts:
        print("â— í•™ìŠµí•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        return

    train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)
    tokenizer = BertTokenizer.from_pretrained('monologg/kobert')
    bert_model = BertModel.from_pretrained('monologg/kobert')

    train_dataset = SummaryDataset(train_texts, train_labels, tokenizer)
    val_dataset = SummaryDataset(val_texts, val_labels, tokenizer)
    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=4)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = KoBERTClassifier(bert_model).to(device)
    optimizer = AdamW(model.parameters(), lr=2e-5)
    criterion = torch.nn.CrossEntropyLoss()

    start_epoch = 0
    train_losses, val_accuracies = [], []
    best_val_acc = 0.0

    # ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° (ì¬ê°œ)
    if resume_checkpoint and os.path.exists(resume_checkpoint):
        model, optimizer, start_epoch, _ = load_checkpoint(model, optimizer, resume_checkpoint, device)
        print(f"ğŸ‰ í•™ìŠµ ì¬ê°œ: {start_epoch+1} ì—í­ë¶€í„° ì‹œì‘")

    print("[2] ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    for epoch in range(start_epoch, 3):
        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
        val_loss, val_acc = eval_model(model, val_loader, criterion, device)
        train_losses.append(train_loss)
        val_accuracies.append(val_acc)

        print(f"Epoch {epoch+1}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}")

        # ì—í­ë³„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        save_checkpoint(model, optimizer, epoch, train_loss, f"rag_classifier_epoch{epoch+1}.pth")

        # ëª¨ë¸ë„ ë³„ë„ë¡œ ì €ì¥
        torch.save(model.state_dict(), "/content/rag_classifier_model.pt")

    print("\nâœ… í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    plt.plot(train_losses, label='train_loss')
    plt.plot(val_accuracies, label='val_acc')
    plt.legend()
    plt.show()

if __name__ == "__main__":

    main(force=False, resume_checkpoint=None)
