import os
import torch
import threading
from flask import Flask, request, jsonify
from flask_ngrok import run_with_ngrok
from typing import List
from dotenv import load_dotenv
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration
from naver_news_crawler import NaverNewsCrawler
from kobert_classifier import predict_prob
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

os.environ["NAVER_CLIENT_ID"] = "9wkHoQIN4RQakh958RQv"
os.environ["NAVER_CLIENT_SECRET"] = "xs9W_ozJ1g"
load_dotenv()

app = Flask(__name__)
run_with_ngrok(app)
crawler = NaverNewsCrawler()

tokenizer = PreTrainedTokenizerFast.from_pretrained("digit82/kobart-summarization")
bart_model = BartForConditionalGeneration.from_pretrained("digit82/kobart-summarization")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
bart_model.to(device)
bart_model.eval()

# ë²¡í„° ì €ì¥ì†Œ
class VectorStore:
    def __init__(self):
        self.vectorizer = TfidfVectorizer(stop_words='english')
        self.doc_vectors = None
        self.documents = []

    def build_index(self, docs: List[str]):
        self.documents = docs
        self.doc_vectors = self.vectorizer.fit_transform(docs) if docs else None

    def search(self, query: str, top_k=3) -> List[str]:
        if self.doc_vectors is None or not self.documents:
             return []
        query_vec = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_vec, self.doc_vectors).flatten()
        top_indices = similarities.argsort()[-top_k:][::-1]

        return [self.documents[i] for i in top_indices if similarities[i] > 0]


vector_store = VectorStore()

def summarize_docs_korean(docs: List[str], max_docs: int = 3) -> List[str]:
    summaries = []
    for doc in docs[:max_docs]:
        try:
            input_ids = tokenizer.encode(doc[:1024], return_tensors="pt", max_length=1024, truncation=True).to(device)
            summary_ids = bart_model.generate(
                input_ids, max_length=128, min_length=32,
                length_penalty=2.0, num_beams=4, early_stopping=True
            )
            summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
            summaries.append(summary)
        except Exception as e:
            summaries.append(f"ìš”ì•½ ì‹¤íŒ¨: {e}")
    return summaries

# RAG ì‘ë‹µ 
def generate_answer(question: str, contexts: List[str]) -> str:
    context_text = " ".join(contexts)
    input_text = f"{context_text} ì§ˆë¬¸: {question}"
    input_ids = tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True).to(device)
    summary_ids = bart_model.generate(input_ids, max_length=128, num_beams=4, early_stopping=True)
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# ì‹ ë¢°ë„ ë¶„ë¦¬
def final_decision(real_prob, fake_prob, top_docs, answer):
    step1_confidence = max(real_prob, fake_prob)
    step2_confidence = min(len(top_docs) / 3, 1.0) if top_docs else 0.0

    if step1_confidence > 0.8 and step2_confidence > 0.6:
        decision = "ğŸŸ¢ ì§„ì§œì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤."
    elif fake_prob > 0.8:
        decision = "ğŸ”´ ê°€ì§œì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤."
    elif not top_docs:
        decision = "âšª ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ì–´ ì§„ìœ„ë¥¼ íŒë‹¨í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."
    else:
        decision = "ğŸŸ¡ í™•ì‹ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¶”ê°€ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤."

    return decision, round(step1_confidence, 3), round(step2_confidence, 3)

# Flask API 
@app.route("/verify", methods=["POST"])
def verify_news():
    try:
        data = request.get_json()
        text = data.get("text", "").strip()
        if not text:
            return jsonify({"error": "í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."}), 400

        probs = predict_prob(text)
        real_prob, fake_prob = probs[0], probs[1]

        keywords = crawler.extract_keywords(text)
        urls = crawler.search_news_urls(keywords)
        docs = crawler.crawl_articles(urls)

        vector_store.build_index(docs)
        top_docs = vector_store.search(text, top_k=3)
        summaries = summarize_docs_korean(top_docs)
        answer = generate_answer(text, top_docs) if top_docs else ""

        decision, step1_conf, step2_conf = final_decision(real_prob, fake_prob, top_docs, answer)

        return jsonify({
            "error": None,
            "real": round(real_prob, 3),
            "fake": round(fake_prob, 3),
            "step1_confidence": step1_conf,
            "step2_confidence": step2_conf,
            "confidence": max(step1_conf, step2_conf),
            "keywords": keywords,
            "urls": urls,
            "summaries": summaries,
            "ragAnswer": answer,
            "finalDecision": decision
        }), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500

def wrap_text_by_sentences(text: str) -> str:
    import re
    sentences = re.split(r'(?<=[.!?]) +', text)
    return "\n".join(sentences)

def wrap_text_by_length(text: str, max_len=80) -> str:
    lines = []
    while len(text) > max_len:
        split_pos = text.rfind(" ", 0, max_len)
        if split_pos == -1:
            split_pos = max_len
        lines.append(text[:split_pos])
        text = text[split_pos:].lstrip()
    lines.append(text)
    return "\n".join(lines)

def format_summaries(summaries: List[str]) -> str:
    formatted = []
    for i, summary in enumerate(summaries, 1):
        wrapped = wrap_text_by_length(summary, max_len=60)
        formatted.append(f"{i}. \n{wrapped}")
    return "\n\n".join(formatted)

# CLI 
def cli_loop():

    print("ë‰´ìŠ¤ ê²€ìƒ‰ ì±—ë´‡ ì‹œì‘í•©ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥í•˜ì„¸ìš”.\n")
    while True:
        text = input(f"\033[33më¬¸ì¥ì„ ì…ë ¥í•´ì£¼ì„¸ìš”: \033[0m").strip()
        if text.lower() == "exit":
            print("ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        if not text:
            print("âš ï¸ ë¹ˆ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.\n")
            continue

        try:
            probs = predict_prob(text)
            if len(probs) < 2:
                print("âš ï¸ ëª¨ë¸ ì¶œë ¥ ì˜¤ë¥˜\n")
                continue
            real_prob, fake_prob = probs[0], probs[1]

            keywords = crawler.extract_keywords(text)
            urls = crawler.search_news_urls(keywords)

            print("ğŸ”‘ í•µì‹¬ í‚¤ì›Œë“œ:", ", ".join(keywords))
            print("ğŸ”— ê´€ë ¨ ê¸°ì‚¬ URL ëª©ë¡:")
            if urls:
                for url in urls:
                    print(f"  - {url}")
            else:
                print("  ì—†ìŒ")

            docs = crawler.crawl_articles(urls)
            if not docs:
                print("\nğŸ’¡ RAG ì‘ë‹µ:")
                print("ì •ë¶€ì˜ ë°œí‘œì™€ ê´€ë ¨ëœ ë‰´ìŠ¤ëŠ” í™•ì¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                continue

            vector_store.build_index(docs)
            top_docs = vector_store.search(text, top_k=3)
            if not top_docs:
                print("\nğŸ’¡ RAG ì‘ë‹µ:")
                print("ê´€ë ¨ ë‰´ìŠ¤ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                continue

            answer = generate_answer(text, top_docs)
            summaries = summarize_docs_korean(top_docs)

            # RAG ê¸°ë°˜ ì‹ ë¢°ë„ (ì˜ˆì‹œë¡œ ê³ ì •ê°’ ì‚¬ìš©)
            rag_confidence = len(top_docs) / 4

            print("\nğŸ’¡ RAG ì‘ë‹µ:")
            print(format_summaries(summaries))
            
            print("\nğŸ“ ìš”ì•½:")
            for i, s in enumerate(summaries, 1):
                print(f"{i}. {wrap_text_by_length(s)}")

            print("\n" + "â”€" * 70)

            print(f"\nğŸ” 1ë‹¨ê³„ ì‹ ë¢°ë„ (ëª¨ë¸ ê¸°ë°˜): {real_prob:.3f}")
            print(f"ğŸ” 2ë‹¨ê³„ ì‹ ë¢°ë„ (RAG ê¸°ë°˜): {rag_confidence:.3f}")

            print(f"\nâœ… ì§„ì§œë‰´ìŠ¤ í™•ë¥ : {real_prob * 100:.2f}%")
            print(f"âŒ ê°€ì§œë‰´ìŠ¤ í™•ë¥ : {fake_prob * 100:.2f}%")

            final_score = real_prob * 0.5 + rag_confidence * 0.5
            final_label = "âœ… ì§„ì§œì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ ê¸°ì‚¬ì…ë‹ˆë‹¤!" if final_score >= 0.5 else "âŒ ê°€ì§œì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ ê¸°ì‚¬ì…ë‹ˆë‹¤!"

            print(f"\nğŸ“Œ ìµœì¢… íŒë‹¨: {final_label}\n")
            print("â”€" * 70 + "\n")

        except Exception as e:
            print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}\n")

# ì‹¤í–‰
if __name__ == "__main__":
    threading.Thread(target=lambda: app.run(host="0.0.0.0", port=8000, debug=False, use_reloader=False)).start()
    cli_loop()
