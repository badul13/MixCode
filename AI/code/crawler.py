import os
import torch
import threading
import pandas as pd
from flask import Flask, request, jsonify
from typing import List
from dotenv import load_dotenv
from transformers import PreTrainedTokenizerFast
from transformers import BertTokenizer
from transformers import BertForSequenceClassification
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration
from naver_news_crawler import NaverNewsCrawler
from transformers import AutoTokenizer
from pyngrok import ngrok
from flask_cors import CORS
from rag_engine import RAGEngine

# RAG ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
vector_store = RAGEngine()

# ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ ë¹Œë“œ (ì´ˆê¸°)
df = pd.read_csv('outputdata.csv')
docs = df['text'].dropna().tolist()
vector_store.build_index(docs)

load_dotenv()

ngrok.set_auth_token("2xwNdP6n13UgVoHisO4aafDb8kq_2J1uW4qGbkwE3CbDztGKn")

app = Flask(__name__)
CORS(app)

crawler = NaverNewsCrawler()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ìš”ì•½ ëª¨ë¸ ë¡œë“œ (KoBART)
tokenizer_kobart = PreTrainedTokenizerFast.from_pretrained("digit82/kobart-summarization")
model_kobart = BartForConditionalGeneration.from_pretrained("digit82/kobart-summarization")
model_kobart.to(device)
model_kobart.eval()

# 1ë‹¨ê³„ KcELECTRA ê¸°ë°˜ ê°€ì§œ ë‰´ìŠ¤ ë¶„ë¥˜ê¸°
class KcElectraClassifier:
    def __init__(self, model_path="news_classifier_model.pt"):
        self.device = device
        self.tokenizer = AutoTokenizer.from_pretrained("monologg/kobert")
        self.model = BertForSequenceClassification.from_pretrained("monologg/kobert", num_labels=2)
        self.model.to(self.device)
        self.model.eval()

    def predict_prob(self, text):
        inputs = self.tokenizer(text, return_tensors="pt", max_length=512, truncation=True, padding=True)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()[0]
        # (ì§„ì§œ:1, ê°€ì§œ:0)
        return probs[1], probs[0]

# 2ë‹¨ê³„ ìš”ì•½ ê¸°ë°˜ KoBERT ë¶„ë¥˜ê¸° (RAGìš©)
class RagKoBertClassifier:
    def __init__(self, model_path="rag_classifier_model.pt"):
        self.device = device
        self.tokenizer = BertTokenizer.from_pretrained("monologg/kobert")
        self.model = BertForSequenceClassification.from_pretrained("monologg/kobert", num_labels=2)
        self.model.to(self.device)
        self.model.eval()

    def predict_rag_prob(self, text, summary_concat):
        input_text = text + " [SEP] " + summary_concat
        inputs = self.tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True, padding=True)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()[0]
        label = int(probs.argmax())
        return probs[1], probs[0], label

# ìš”ì•½ í•¨ìˆ˜
def summarize_docs_korean(docs: List[str], max_docs: int = 3) -> List[str]:
    summaries = []
    for doc in docs[:max_docs]:
        try:
            input_ids = tokenizer_kobart.encode(doc[:1024], return_tensors="pt", max_length=1024, truncation=True).to(device)
            summary_ids = model_kobart.generate(
                input_ids,
                max_length=128,
                min_length=64,
                length_penalty=2.0,
                num_beams=4,
                early_stopping=True,
            )
            summary = tokenizer_kobart.decode(summary_ids[0], skip_special_tokens=True)
            summaries.append(summary)
        except Exception as e:
            summaries.append(f"ìš”ì•½ ì‹¤íŒ¨: {e}")
    return summaries

def clean_summary(text: str) -> str:
    awkward_starts = ["í–ˆë‹¤.", "ì´ë‹¤.", "ìš”?"]
    text = text.strip()
    for aw in awkward_starts:
        if text.startswith(aw):
            text = text[len(aw):].strip()
    if text and not text.endswith('.'):
        text += '.'
    return text

# 1ë‹¨ê³„, 2ë‹¨ê³„ í™•ë¥  í•©ì¹˜ëŠ” í•¨ìˆ˜ (ê°€ì¤‘ì¹˜ ì ìš© ì˜ˆì‹œ)
def combined_prob(real1, fake1, real2, fake2, weight=0.5):
    combined_real = real1 * (1 - weight) + real2 * weight
    combined_fake = fake1 * (1 - weight) + fake2 * weight
    s = combined_real + combined_fake
    if s == 0:
        return 0.0, 0.0
    return combined_real / s, combined_fake / s

# ëª¨ë¸ ì´ˆê¸°í™” (ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ í¬í•¨)
kc_electra_model = KcElectraClassifier("news_classifier_model.pt")
rag_kobert_model = RagKoBertClassifier("rag_classifier_model.pt")

def generate_answer(query: str, docs: List[str]):
    answer = " ".join(docs[:3])
    confidence = 0.9  # í•„ìš”ì‹œ ìˆ˜ì •
    return answer, confidence

@app.route("/verify", methods=["POST"])
def verify_news():
    try:
        data = request.get_json()
        text = data["text"]

        # 1ë‹¨ê³„ ì˜ˆì¸¡
        real_prob_1, fake_prob_1 = kc_electra_model.predict_prob(text)

        keywords = crawler.extract_keywords(text)
        urls = crawler.search_news_urls(keywords)
        docs = crawler.crawl_articles(urls)

        if not docs:
            return jsonify({
                "error": "ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ ì—†ìŒ",
                "real_1": round(real_prob_1, 3),
                "fake_1": round(fake_prob_1, 3),
                "keywords": keywords,
                "urls": urls,
                "ragAnswer": "",
                "confidence": 0.0,
                "summaries": []
            }), 200

        # ì¸ë±ìŠ¤ ì¬êµ¬ì¶•
        vector_store.build_index(docs)
        top_docs = vector_store.search(text, top_k=3)

        answer, rag_confidence = generate_answer(text, top_docs)
        summaries = summarize_docs_korean(top_docs)
        summary_concat = " ".join(summaries)

        # 2ë‹¨ê³„ RAG ë¶„ë¥˜ê¸° ì˜ˆì¸¡
        rag_real_prob, rag_fake_prob, rag_label = rag_kobert_model.predict_rag_prob(text, summary_concat)

        combined_real, combined_fake = combined_prob(
            real_prob_1, fake_prob_1, rag_real_prob, rag_fake_prob, weight=0.5
        )

        return jsonify({
            "real_1": float(round(real_prob_1, 3)),
            "fake_1": float(round(fake_prob_1, 3)),
            "rag_real": float(round(rag_real_prob, 3)),
            "rag_fake": float(round(rag_fake_prob, 3)),
            "combined_real": float(round(combined_real, 3)),
            "combined_fake": float(round(combined_fake, 3)),
            "rag_confidence": float(round(rag_confidence, 3)),
            "label": int(rag_label),
            "keywords": keywords,
            "urls": urls,
            "ragAnswer": answer,
            "confidence": float(round(rag_confidence, 3)),
            "summaries": summaries
        }), 200


    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route("/ngrok_url")
def get_ngrok_url():
    return jsonify({"ngrok_url": public_url})

def wrap_text_by_length(text: str, max_len=80) -> str:
    lines = []
    while len(text) > max_len:
        split_pos = text.rfind(" ", 0, max_len)
        if split_pos == -1:
            split_pos = max_len
        lines.append(text[:split_pos])
        text = text[split_pos:].lstrip()
    lines.append(text)
    return "\n".join(lines)

def format_summaries(summaries: List[str]) -> str:
    formatted = []
    for i, summary in enumerate(summaries, 1):
        wrapped = wrap_text_by_length(summary, max_len=60)
        formatted.append(f"{i}. \n{wrapped}")
    return "\n\n".join(formatted)

def final_decision(real_prob_1, fake_prob_1, rag_real_prob, rag_fake_prob, threshold=0.5):
    combined_real, combined_fake = combined_prob(real_prob_1, fake_prob_1, rag_real_prob, rag_fake_prob)
    if combined_real >= threshold:
        return 1, combined_real
    else:
        return 0, combined_fake

def cli_loop():
    print("ë‰´ìŠ¤ ì§„ìœ„ ì˜ˆì¸¡ + RAG ê¸°ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰ ì±—ë´‡ ì‹œì‘. ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥\n")
    while True:
        text = input("ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”: ")
        if text.lower() == "exit":
            print("ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        try:
            real_prob_1, fake_prob_1 = kc_electra_model.predict_prob(text)
            keywords = crawler.extract_keywords(text)
            urls = crawler.search_news_urls(keywords)
            docs = crawler.crawl_articles(urls)

            if not docs:
                print("âš ï¸ ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ\n")
                continue

            vector_store.build_index(docs)
            top_docs = vector_store.search(text, top_k=3)

            answer, rag_confidence = generate_answer(text, top_docs)
            summaries = summarize_docs_korean(top_docs)
            summary_concat = " ".join(summaries)

            rag_real_prob, rag_fake_prob, rag_label = rag_kobert_model.predict_rag_prob(text, summary_concat)

            combined_real, combined_fake = combined_prob(
                real_prob_1, fake_prob_1, rag_real_prob, rag_fake_prob, weight=0.5
            )

            print("ğŸ”‘ í•µì‹¬ í‚¤ì›Œë“œ:", ", ".join(keywords))
            print("ğŸ”— ê´€ë ¨ ê¸°ì‚¬ URL ëª©ë¡:", urls)
            print(f"\nğŸ’¡ RAG ì‘ë‹µ:\n{answer}")
            print(f"ğŸ” ì‹ ë¢°ë„: {rag_confidence:.3f}")
            print("ğŸ“ ìš”ì•½:\n")
            for i, summary in enumerate(summaries, 1):
                s_clean = clean_summary(summary)
                if s_clean:
                    print(f"{i}. {s_clean}\n")

            print("ğŸ“Š ì‹ ë¢°ë„ ë¶„ì„ ê²°ê³¼:")
            print(f"- KoBERT ë¶„ë¥˜ ê²°ê³¼: {'ê°€ì§œë‰´ìŠ¤' if fake_prob_1 >= 0.5 else 'ì§„ì§œë‰´ìŠ¤'} ({fake_prob_1:.3f} í™•ë¥ )")
            print(f"- RAG ê¸°ë°˜ ë¶„ë¥˜ ê²°ê³¼: {'ê°€ì§œë‰´ìŠ¤' if rag_fake_prob >= 0.5 else 'ì§„ì§œë‰´ìŠ¤'} ({rag_confidence:.3f} í™•ë¥ )\n")

            print(f"\nğŸ§¾ ì§„ì§œë‰´ìŠ¤ í™•ë¥ : {combined_real * 100:.2f}%")
            print(f"ğŸ§¾ ê°€ì§œë‰´ìŠ¤ í™•ë¥ : {combined_fake * 100:.2f}%")

            final_label, final_confidence = final_decision(real_prob_1, fake_prob_1, rag_real_prob, rag_fake_prob)
            if final_label == 1:
                print("\n ìµœì¢… íŒë‹¨: ì§„ì§œì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ ê¸°ì‚¬ì…ë‹ˆë‹¤!")
            else:
                print("\n ìµœì¢… íŒë‹¨: ê°€ì§œì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ ê¸°ì‚¬ì…ë‹ˆë‹¤!")

            print("\n" + "=" * 50 + "\n")

        except Exception as e:
            print(f"â— ì˜¤ë¥˜ ë°œìƒ: {e}")

def run_flask():
    app.run(host="0.0.0.0", port=8070)

if __name__ == "__main__":
    port = 8070
    public_url = ngrok.connect(port).public_url
    print(f" * ngrok tunnel URL: {public_url}")

    # Flask ì„œë²„ ë³„ë„ ìŠ¤ë ˆë“œë¡œ ì‹¤í–‰
    threading.Thread(target=run_flask).start()

    # CLI ì‹¤í–‰
    cli_loop()
