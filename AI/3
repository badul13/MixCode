import os
import torch
import threading
from flask import Flask, request, jsonify
from typing import List
from dotenv import load_dotenv
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration

from naver_news_crawler import NaverNewsCrawler
from kobert_classifier import predict_prob
from rag_engine import vector_store, generate_answer

load_dotenv()

app = Flask(__name__)
crawler = NaverNewsCrawler()

# ìš”ì•½ ëª¨ë¸ ë¡œë“œ
tokenizer = PreTrainedTokenizerFast.from_pretrained("digit82/kobart-summarization")
model = BartForConditionalGeneration.from_pretrained("digit82/kobart-summarization")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

def summarize_docs_korean(docs: List[str], max_docs: int = 3) -> List[str]:
    summaries = []
    for doc in docs[:max_docs]:
        try:
            input_ids = tokenizer.encode(doc[:1024], return_tensors="pt", max_length=1024, truncation=True).to(device)
            summary_ids = model.generate(input_ids, max_length=128, min_length=32, length_penalty=2.0, num_beams=4, early_stopping=True)
            summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
            summaries.append(summary)
        except Exception as e:
            summaries.append(f"ìš”ì•½ ì‹¤íŒ¨: {e}")
    return summaries

@app.route("/verify", methods=["POST"])
def verify_news():
    try:
        data = request.get_json()
        text = data["text"]

        probs = predict_prob(text)
        real_prob, fake_prob = probs[0], probs[1]

        keywords = crawler.extract_keywords(text)
        urls = crawler.search_news_urls(keywords)
        docs = crawler.crawl_articles(urls)

        if not docs:
            return jsonify({
                "error": "ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ ì—†ìŒ",
                "real": round(real_prob, 3),
                "fake": round(fake_prob, 3),
                "keywords": keywords,
                "urls": urls,
                "ragAnswer": "",
                "confidence": 0.0,
                "summaries": []
            }), 200

        vector_store.build_index(docs)
        top_docs = vector_store.search(text, top_k=3)
        answer, confidence = generate_answer(text, top_docs)
        summaries = summarize_docs_korean(top_docs)

        return jsonify({
            "real": round(real_prob, 3),
            "fake": round(fake_prob, 3),
            "keywords": keywords,
            "urls": urls,
            "ragAnswer": answer,
            "confidence": round(confidence, 3),
            "summaries": summaries
        }), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500

def cli_loop():
    print("ë‰´ìŠ¤ ì§„ìœ„ ì˜ˆì¸¡ + RAG ê¸°ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰ ì±—ë´‡ ì‹œì‘. ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥\n")
    while True:
        text = input("ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”: ")
        if text.lower() == "exit":
            print("ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        try:
            probs = predict_prob(text)
            real_prob, fake_prob = probs[0], probs[1]
            keywords = crawler.extract_keywords(text)
            urls = crawler.search_news_urls(keywords)
            print(f"\nâœ… ì§„ì§œë‰´ìŠ¤ í™•ë¥ : {real_prob * 100:.2f}%")
            print(f"âŒ ê°€ì§œë‰´ìŠ¤ í™•ë¥ : {fake_prob * 100:.2f}%")
            print("ğŸ”‘ í‚¤ì›Œë“œ:", ", ".join(keywords))
            print("ğŸ”— URL ëª©ë¡:", urls)

            docs = crawler.crawl_articles(urls)
            if not docs:
                print("âš ï¸ ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ\n")
                continue

            vector_store.build_index(docs)
            top_docs = vector_store.search(text, top_k=3)
            answer, confidence = generate_answer(text, top_docs)
            summaries = summarize_docs_korean(top_docs)

            print(f"\nğŸ’¡ RAG ì‘ë‹µ:\n{answer}")
            print(f"ğŸ” ì‹ ë¢°ë„: {confidence:.3f}")
            print("ğŸ“ ìš”ì•½:")
            for i, s in enumerate(summaries, 1):
                print(f"{i}. {s}")
            print("\n=====================\n")

        except Exception as e:
            print(f"â— ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    threading.Thread(target=lambda: app.run(host="0.0.0.0", port=8000, debug=False, use_reloader=False)).start()
    cli_loop()
