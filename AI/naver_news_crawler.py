import os
import re
import requests
from typing import List
from urllib.parse import quote
from bs4 import BeautifulSoup

class NaverNewsCrawler:
    def __init__(self):
        self.client_id = os.getenv("NAVER_CLIENT_ID")
        self.client_secret = os.getenv("NAVER_CLIENT_SECRET")
        self.headers = {
            "X-Naver-Client-Id": self.client_id,
            "X-Naver-Client-Secret": self.client_secret
        }

    def extract_keywords(self, text: str) -> List[str]:
        words = re.findall(r'\b[ê°€-í£]{2,}\b', text)
        return list(set(words))[:5]

    def search_news_urls(self, keywords: List[str]) -> List[str]:
        if not keywords:
            return []

        query = quote(" ".join(keywords))
        url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display=5&sort=sim"

        try:
            res = requests.get(url, headers=self.headers, timeout=5)
            if res.status_code != 200:
                print("ğŸ”´ ë„¤ì´ë²„ API ìš”ì²­ ì‹¤íŒ¨:", res.status_code, res.text)
                return []

            items = res.json().get("items", [])
            links = [item["link"] for item in items if "news.naver.com" in item["link"]]
            return list(set(links))

        except Exception as e:
            print(f"ğŸ”´ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def crawl_articles(self, urls: List[str]) -> List[str]:
        articles = []
        for url in urls:
            try:
                res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5)
                soup = BeautifulSoup(res.text, "html.parser")

                selectors = [
                    "div#newsct_article",   # ì¼ë°˜ ë‰´ìŠ¤ (ê°€ì¥ í”í•¨)
                    "div#articleBodyContents",  # ì˜›ë‚  ëª¨ë°”ì¼ í˜ì´ì§€
                    "div#articeBody",       # ì¼ë¶€ êµ¬í˜• ë‰´ìŠ¤
                    "div#newsEndContents",  # ê¸°íƒ€
                    "div#dic_area",         # ì •ì¹˜/ì˜¤í”¼ë‹ˆì–¸ ë“±
                    "div.article_body"      # ë„¤ì´ë²„ ìŠ¤í¬ì¸  ë“±
                ]

                article_text = ""
                for sel in selectors:
                    container = soup.select_one(sel)
                    if container:
                        article_text = container.get_text(separator=" ", strip=True)
                        break

                if len(article_text) > 100:
                    articles.append(article_text)
                else:
                    print(f"â— ë³¸ë¬¸ ë„ˆë¬´ ì§§ìŒ or ì¶”ì¶œ ì‹¤íŒ¨: {url}")

            except Exception as e:
                print(f"ğŸ”´ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨ {url}: {e}")
                continue

        return articles
